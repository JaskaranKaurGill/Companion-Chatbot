{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using Titles BioBERT CLassification.ipynb",
      "provenance": [],
      "mount_file_id": "190_oLAfFVYeCorjlgWOicBdGth8bRf6_",
      "authorship_tag": "ABX9TyNPvIVxANxh+HnVuI1rTOI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaskaranKaurGill/Companion-Chatbot/blob/master/Using_Titles_BioBERT_CLassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUjIa8sq0iZI",
        "outputId": "53854da6-2147-4919-e5c0-442aeeec7cb9"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-28 11:38:00--  https://docs.google.com/uc?export=download&confirm=H9h5&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.113, 108.177.111.100, 108.177.111.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "--2021-11-28 11:38:00--  https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-14-68-docs.googleusercontent.com (doc-14-68-docs.googleusercontent.com)... 173.194.197.132, 2607:f8b0:4001:c1b::84\n",
            "Connecting to doc-14-68-docs.googleusercontent.com (doc-14-68-docs.googleusercontent.com)|173.194.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=qinbgluav6it4&continue=https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=vdmqfl95et897sfhgt9otl66mumdb82t [following]\n",
            "--2021-11-28 11:38:00--  https://docs.google.com/nonceSigner?nonce=qinbgluav6it4&continue=https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=vdmqfl95et897sfhgt9otl66mumdb82t\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=qinbgluav6it4&user=04682528429876795100Z&hash=lc6j1bhnd79hv8gdnjm5eta37nbdao4r [following]\n",
            "--2021-11-28 11:38:00--  https://doc-14-68-docs.googleusercontent.com/docs/securesc/4541ta5g0o0hkta4gmv4dilv2ubqjcl9/rfpfk1e8l2t76cribfvt9l6o5mc0oikb/1638099450000/13799006341648886493/04682528429876795100Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=qinbgluav6it4&user=04682528429876795100Z&hash=lc6j1bhnd79hv8gdnjm5eta37nbdao4r\n",
            "Connecting to doc-14-68-docs.googleusercontent.com (doc-14-68-docs.googleusercontent.com)|173.194.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 401403346 (383M) [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights     100%[===================>] 382.81M   187MB/s    in 2.0s    \n",
            "\n",
            "2021-11-28 11:38:03 (187 MB/s) - ‘biobert_weights’ saved [401403346/401403346]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWFLmjok0vU7",
        "outputId": "9690bec7-7845-43e4-f67f-0e9173584099"
      },
      "source": [
        "!pip install pytorch_transformers\n",
        "!pip install transformers\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.14-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.10.0.2)\n",
            "Collecting botocore<1.24.0,>=1.23.14\n",
            "  Downloading botocore-1.23.14-py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.14->boto3->pytorch_transformers) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.14->boto3->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.14 botocore-1.23.14 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmZ4EVT40xnj",
        "outputId": "ce4cb488-8032-44c4-8e77-97b1ac75636d"
      },
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF1TPu4601E_",
        "outputId": "4e48aeeb-aab4-4b24-837f-e3a62186c906"
      },
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzLpe8Hq02fE",
        "outputId": "307c0d08-a804-4ed3-90c7-519251735553"
      },
      "source": [
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgftHM1R04DQ"
      },
      "source": [
        "from pytorch_transformers import BertModel\n",
        "model = BertModel.from_pretrained('biobert_v1.1_pubmed')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXbnoVBK05ab",
        "outputId": "c1405636-9b04-4c4a-c1eb-6b535669ad7f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biobert_v1.1_pubmed  biobert_weights  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-qfztdm08jN"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('shuffled_final_df.xlsx')\n",
        "df.head()\n",
        "df = df[['Title','Abstract','target']]\n",
        "df = df.dropna()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3WYl7jrBcWH"
      },
      "source": [
        "import re\n",
        "def cleanText(text):\n",
        "        \n",
        "    text = text.replace('\\\\n','')\n",
        "    text = text.replace('\\\\','')\n",
        "    #text = text.replace('\\t', '')\n",
        "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
        "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
        "                ' __url__ ',text) #remove urls\n",
        "    #text = re.sub('\\'','',text)\n",
        "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
        "    #text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.replace('\\t', '')\n",
        "    text = text.replace('\\n', '')\n",
        "    return text\n",
        "Abstracts = []\n",
        "for text in df['Abstract']:\n",
        "    Abstracts.append(cleanText(text))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfT43zTy1mc4",
        "outputId": "d36b9686-449b-4730-b059-9aff15b26826"
      },
      "source": [
        "DATA_DIR=\".\"\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "# install BERT\n",
        "!pip install pytorch_pretrained_bert pytorch-nlp\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "#torch.cuda.get_device_name(0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.20.14)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.14 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.23.14)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.14->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.14->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.14->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Installing collected packages: pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B7NgyXQ13O9"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#sentiment is positive and negative we need to convert it to 0,1\n",
        "le = LabelEncoder()\n",
        "df[\"target\"] = le.fit_transform(df[\"target\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTwqQEwY4QOp",
        "outputId": "47f787a5-c97b-4325-a580-9bf93813d633"
      },
      "source": [
        "Abstracts = df['Title']\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(cleanText(t))+['[SEP]'] , Abstracts))\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'activation', 'of', 'es', '##cher', '##ichi', '##a', 'co', '##li', 'r', '##rna', 'transcription', 'by', 'fi', '##s', 'during', 'a', 'growth', 'cycle', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioBb4C_74ULb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d427057-79b0-475e-dd89-a3d0da0e5825"
      },
      "source": [
        "how_many = 0\n",
        "for abstract in df['Abstract']:\n",
        "  if(len(cleanText(abstract))<=512):\n",
        "    how_many = how_many + 1\n",
        "print(how_many)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDlGMP-14eyR"
      },
      "source": [
        "classes = list(df['target'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAAyXgu559so"
      },
      "source": [
        "max_len = 100\n",
        "input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQye12y6UAZ"
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdC8A2lRrWPb",
        "outputId": "44617cb0-178a-405d-95f5-66e8bd1a7b02"
      },
      "source": [
        "len(input_ids[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZYoZDBk6qsN"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP5I_7Ih6rNp"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, classes, \n",
        "                                                            random_state=2020, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.2)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGFioVzj8HNv",
        "outputId": "a7199b72-55ef-42c5-e143-08c2ce61522e"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"biobert_v1.1_pubmed\", num_labels=2)#binary classification\n",
        "model.cuda()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzLheFay8JJH",
        "outputId": "beb12220-2f1b-46ba-a130-b415fd020f24"
      },
      "source": [
        "# BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = 4"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaPy-6o12zqD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "lJmo0A3C8V2b",
        "outputId": "25e39759-0a71-41c0-dc59-f8cd25a7005b"
      },
      "source": [
        "# BERT training loop\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## TRAINING\n",
        "  torch.cuda.empty_cache()\n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "       \n",
        "  ## VALIDATION\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# plot training performance\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.23834342670937378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [01:12<03:38, 72.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9618055555555556\n",
            "Train loss: 0.06846637985048194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [02:27<02:27, 73.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9637586805555556\n",
            "Train loss: 0.01988482691037158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [03:42<01:14, 74.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9676649305555556\n",
            "Train loss: 0.010516161397875597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 4/4 [04:57<00:00, 74.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9657118055555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT4LWQ0z93uh"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tls88q6XAWa6",
        "outputId": "ab7f9c2c-2fa4-46af-f33d-bf83bbb2bd43"
      },
      "source": [
        "for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9598524305555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Run79P47AZlq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSzPu2PDAbsC",
        "outputId": "a6360a3f-99f3-413b-e330-0c274bdcbb98"
      },
      "source": [
        "for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
        "      logits = logits.detach().cpu().numpy()  \n",
        "      print(logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-3.9855583  4.2861137]\n",
            " [-3.9549735  4.2927947]\n",
            " [ 2.9904273 -3.991263 ]\n",
            " [-3.6963713  3.9528513]\n",
            " [-3.9357936  4.2624006]\n",
            " [ 3.2469459 -4.429582 ]\n",
            " [ 3.169044  -4.205132 ]\n",
            " [ 3.2640972 -4.2533   ]\n",
            " [ 3.0694711 -4.082383 ]\n",
            " [ 3.2507718 -4.4401855]\n",
            " [-3.655454   3.9838357]\n",
            " [ 3.1892037 -4.2346997]\n",
            " [ 3.1176307 -4.181471 ]\n",
            " [ 3.2427707 -4.3988924]\n",
            " [ 3.3132508 -4.4955006]\n",
            " [-4.0052423  4.3291554]\n",
            " [ 3.1535673 -4.248223 ]\n",
            " [-3.9959354  4.33079  ]\n",
            " [ 3.261915  -4.414614 ]\n",
            " [-3.929769   4.27026  ]\n",
            " [ 3.3188996 -4.36807  ]\n",
            " [-4.014559   4.3530836]\n",
            " [ 2.4089046 -3.259534 ]\n",
            " [ 3.222236  -4.3073425]\n",
            " [ 3.229653  -4.289955 ]\n",
            " [-4.0115933  4.3471146]\n",
            " [ 3.3748486 -4.5728526]\n",
            " [-4.0013967  4.309912 ]\n",
            " [ 3.1488085 -4.1703925]\n",
            " [ 3.2804427 -4.403388 ]\n",
            " [-4.0019727  4.3418217]\n",
            " [-3.2580976  3.4712772]\n",
            " [ 3.3699987 -4.509061 ]\n",
            " [ 1.0933951 -1.5356572]\n",
            " [-3.963366   4.2784004]\n",
            " [-3.9217448  4.2490487]\n",
            " [ 2.3547418 -3.2216558]\n",
            " [ 3.1400428 -4.230242 ]\n",
            " [-2.1489205  2.2680924]\n",
            " [-3.9809582  4.3109617]\n",
            " [-3.8521097  4.1516285]\n",
            " [-3.9330606  4.254446 ]\n",
            " [-3.905449   4.20479  ]\n",
            " [-4.031551   4.349547 ]\n",
            " [ 3.264422  -4.3838644]\n",
            " [-3.9775853  4.320215 ]\n",
            " [-3.985141   4.30083  ]\n",
            " [ 3.20818   -4.3288183]\n",
            " [ 3.185866  -4.258615 ]\n",
            " [-0.6940953  0.6118854]\n",
            " [ 2.856841  -3.8275506]\n",
            " [-3.8692865  4.148251 ]\n",
            " [-3.9554942  4.2691836]\n",
            " [ 3.3019311 -4.481216 ]\n",
            " [-3.9728246  4.2885933]\n",
            " [-3.902311   4.2176127]\n",
            " [ 1.3726794 -1.9332755]\n",
            " [ 3.2812757 -4.37924  ]\n",
            " [-3.6851594  4.0008307]\n",
            " [-4.0010443  4.3406487]\n",
            " [ 1.1101787 -1.6215076]\n",
            " [-3.9741926  4.319434 ]\n",
            " [ 3.0618656 -4.1210737]\n",
            " [-3.4190512  3.7511647]]\n",
            "[[ 3.211393   -4.3719893 ]\n",
            " [ 2.9466476  -3.9686403 ]\n",
            " [-4.024901    4.3426023 ]\n",
            " [-3.8063047   4.12661   ]\n",
            " [ 3.28601    -4.416591  ]\n",
            " [-4.0011926   4.327254  ]\n",
            " [ 3.288361   -4.352084  ]\n",
            " [ 3.1018693  -4.14057   ]\n",
            " [-3.1230373   3.4752116 ]\n",
            " [-3.8894272   4.1764555 ]\n",
            " [ 3.2306757  -4.429275  ]\n",
            " [ 3.342327   -4.4765964 ]\n",
            " [ 3.0809255  -4.1193333 ]\n",
            " [ 3.2663898  -4.426078  ]\n",
            " [-3.9765584   4.2806683 ]\n",
            " [-4.0042224   4.3354907 ]\n",
            " [ 2.4407878  -3.2995632 ]\n",
            " [ 3.2902849  -4.473537  ]\n",
            " [ 2.3614438  -3.212794  ]\n",
            " [ 3.2428324  -4.3954263 ]\n",
            " [ 3.2804925  -4.4416423 ]\n",
            " [ 0.5522369  -0.78759605]\n",
            " [-3.110817    3.2875352 ]\n",
            " [-3.5183246   3.7370584 ]\n",
            " [ 2.7522616  -3.7224638 ]\n",
            " [ 3.3444972  -4.569018  ]\n",
            " [ 3.1819487  -4.256947  ]\n",
            " [ 3.1098075  -4.211299  ]\n",
            " [-3.903214    4.1679006 ]\n",
            " [-2.8041627   3.061515  ]\n",
            " [-3.9841073   4.310329  ]\n",
            " [ 3.129471   -4.191031  ]\n",
            " [ 3.1643226  -4.2051086 ]\n",
            " [-3.9435873   4.248705  ]\n",
            " [ 3.2054503  -4.258682  ]\n",
            " [-3.9966633   4.3405294 ]\n",
            " [-3.9987247   4.3050475 ]\n",
            " [ 3.2852485  -4.364073  ]\n",
            " [-3.999332    4.308611  ]\n",
            " [-3.1691456   3.5980773 ]\n",
            " [ 3.2551372  -4.4469714 ]\n",
            " [ 3.2066288  -4.302569  ]\n",
            " [ 3.2983897  -4.496331  ]\n",
            " [ 3.2460997  -4.3346515 ]\n",
            " [-3.624893    3.9345708 ]\n",
            " [-3.9324894   4.212259  ]\n",
            " [-3.8744226   4.1751714 ]\n",
            " [ 3.2054508  -4.3123856 ]\n",
            " [ 3.194078   -4.3936477 ]\n",
            " [ 3.1769822  -4.27703   ]\n",
            " [ 3.085655   -4.140819  ]\n",
            " [-3.8282032   4.106173  ]\n",
            " [-3.898254    4.204353  ]\n",
            " [ 2.988442   -4.0158534 ]\n",
            " [-3.8935819   4.1974053 ]\n",
            " [-4.0182605   4.3234315 ]\n",
            " [-3.9281642   4.224116  ]\n",
            " [-4.0215516   4.3505483 ]\n",
            " [-3.9566066   4.316096  ]\n",
            " [-3.7352858   4.0491815 ]\n",
            " [-0.56718844  0.5680362 ]\n",
            " [-3.97699     4.280994  ]\n",
            " [-3.9238312   4.2741714 ]\n",
            " [-3.974056    4.27787   ]]\n",
            "[[-3.9900265  4.3080106]\n",
            " [ 3.2361724 -4.376738 ]\n",
            " [-2.8588     3.1890833]\n",
            " [-3.9963315  4.321849 ]\n",
            " [-3.655769   3.8871398]\n",
            " [-3.3956223  3.6785052]\n",
            " [-4.0234623  4.341247 ]\n",
            " [ 3.277092  -4.3909016]\n",
            " [ 3.3705008 -4.5475016]\n",
            " [ 3.2179043 -4.373273 ]\n",
            " [-4.0225277  4.362084 ]\n",
            " [ 3.1400428 -4.230242 ]\n",
            " [ 3.2386518 -4.3365884]\n",
            " [-4.035322   4.3352866]\n",
            " [ 3.2156863 -4.306221 ]\n",
            " [-3.9719684  4.28562  ]\n",
            " [-3.9583192  4.223775 ]\n",
            " [-3.9937043  4.343971 ]\n",
            " [ 3.2399974 -4.295073 ]\n",
            " [ 3.194344  -4.235679 ]\n",
            " [ 3.2323127 -4.3807635]\n",
            " [-3.9860742  4.300202 ]\n",
            " [ 3.2701662 -4.427064 ]\n",
            " [-3.2973897  3.5997188]\n",
            " [ 3.014502  -4.068072 ]\n",
            " [ 3.214586  -4.342584 ]\n",
            " [ 3.0404816 -4.1493626]\n",
            " [-4.025887   4.3566055]\n",
            " [-3.8785648  4.1777925]\n",
            " [-3.2363703  3.5823414]\n",
            " [ 3.135996  -4.1209908]\n",
            " [-3.8593004  4.111974 ]\n",
            " [ 2.937993  -4.0286207]\n",
            " [ 3.3233569 -4.44337  ]\n",
            " [-3.9878979  4.2696795]\n",
            " [-3.9464135  4.257388 ]\n",
            " [-4.013244   4.3406444]\n",
            " [-3.9852872  4.2992506]\n",
            " [ 3.0865664 -4.153029 ]\n",
            " [-3.8959315  4.24576  ]\n",
            " [ 3.274759  -4.403203 ]\n",
            " [-3.993846   4.3081803]\n",
            " [-3.921614   4.249489 ]\n",
            " [ 3.2651844 -4.362892 ]\n",
            " [-2.8026211  2.8833215]\n",
            " [ 3.2367852 -4.3347616]\n",
            " [ 1.7705    -2.5145228]\n",
            " [ 3.3738735 -4.5269494]\n",
            " [-3.4848173  3.8088286]\n",
            " [-3.761938   4.0537286]\n",
            " [ 3.2340539 -4.3820915]\n",
            " [-3.9481432  4.2476277]\n",
            " [ 2.8628418 -3.790029 ]\n",
            " [ 3.2703354 -4.4557385]\n",
            " [-3.9333541  4.2424   ]\n",
            " [-4.0070853  4.3392987]\n",
            " [-3.9146369  4.216191 ]\n",
            " [-3.6105604  3.9500303]\n",
            " [-3.9896038  4.320299 ]\n",
            " [ 3.324393  -4.464639 ]\n",
            " [-3.259184   3.4769688]\n",
            " [ 3.132542  -4.139411 ]\n",
            " [-3.3202217  3.6531854]\n",
            " [-1.5300776  1.4574466]]\n",
            "[[-3.958272   4.28033  ]\n",
            " [-3.8952777  4.189882 ]\n",
            " [ 3.2133472 -4.1578937]\n",
            " [-3.9432106  4.2264395]\n",
            " [ 2.9332006 -3.8082554]\n",
            " [-3.8991725  4.1581755]\n",
            " [-3.993642   4.314171 ]\n",
            " [ 3.015989  -4.0843534]\n",
            " [ 2.6141968 -3.4995744]\n",
            " [ 3.2339332 -4.378341 ]\n",
            " [ 3.226422  -4.409298 ]\n",
            " [ 3.3187263 -4.5110426]\n",
            " [-3.9314716  4.2700863]\n",
            " [ 3.3565092 -4.451166 ]\n",
            " [ 2.1079443 -2.6411293]\n",
            " [ 3.117687  -4.257169 ]\n",
            " [-3.732384   4.1263666]\n",
            " [ 3.3072832 -4.4777117]\n",
            " [-1.3400342  1.3874809]\n",
            " [-3.4428692  3.6616287]\n",
            " [ 3.2193043 -4.2267933]\n",
            " [ 3.3337624 -4.5108504]\n",
            " [-3.9567251  4.277141 ]\n",
            " [ 3.2459605 -4.396507 ]\n",
            " [ 3.3743417 -4.540999 ]\n",
            " [ 3.1870134 -4.2359395]\n",
            " [ 3.0681167 -4.046408 ]\n",
            " [-3.9960055  4.322532 ]\n",
            " [ 3.2465477 -4.447594 ]\n",
            " [-4.001308   4.342563 ]\n",
            " [ 2.9012249 -4.0382648]\n",
            " [-3.8140156  4.1227   ]\n",
            " [-3.8693402  4.1792717]\n",
            " [-3.9592886  4.235709 ]\n",
            " [-3.9988391  4.3086963]\n",
            " [ 3.3799925 -4.532439 ]\n",
            " [-3.9428961  4.299364 ]\n",
            " [-3.8495836  4.1679993]\n",
            " [-3.9763813  4.2847886]\n",
            " [ 3.2536783 -4.43254  ]\n",
            " [-3.781609   4.0340514]\n",
            " [-3.9409387  4.2914863]\n",
            " [-3.9793503  4.265785 ]\n",
            " [ 3.219547  -4.239973 ]\n",
            " [-3.9578052  4.3141737]\n",
            " [-3.9144137  4.212901 ]\n",
            " [-4.034438   4.3534555]\n",
            " [-3.902454   4.2345433]\n",
            " [-4.002988   4.339622 ]\n",
            " [-2.6887314  3.0364156]\n",
            " [-3.9088242  4.175101 ]\n",
            " [ 3.274789  -4.344367 ]\n",
            " [-3.8952005  4.2060795]\n",
            " [ 3.1779919 -4.203114 ]\n",
            " [-4.0298343  4.3294115]\n",
            " [ 3.2255094 -4.2179117]\n",
            " [-3.9566522  4.229422 ]\n",
            " [ 3.2613957 -4.354778 ]\n",
            " [ 3.2785132 -4.414626 ]\n",
            " [-3.8860986  4.1773043]\n",
            " [ 3.2726185 -4.4565997]\n",
            " [-2.8023334  3.0269551]\n",
            " [-2.4608285  2.5469897]\n",
            " [-3.8495495  4.076337 ]]\n",
            "[[-3.9682252   4.2456884 ]\n",
            " [ 3.2981255  -4.36916   ]\n",
            " [-3.800604    4.1524663 ]\n",
            " [ 3.0695894  -4.0159955 ]\n",
            " [ 3.1995876  -4.340845  ]\n",
            " [ 3.353131   -4.440874  ]\n",
            " [-3.756153    4.0740604 ]\n",
            " [ 3.3005574  -4.316081  ]\n",
            " [ 3.32356    -4.550417  ]\n",
            " [-4.023051    4.355306  ]\n",
            " [ 3.2784796  -4.453992  ]\n",
            " [-1.5575589   1.5096836 ]\n",
            " [ 3.3098452  -4.508452  ]\n",
            " [-3.915035    4.207005  ]\n",
            " [ 2.4053576  -3.2668562 ]\n",
            " [ 0.75592536 -1.1880182 ]\n",
            " [ 3.2007215  -4.3155155 ]\n",
            " [ 3.2176971  -4.360804  ]\n",
            " [-2.8483226   3.1846852 ]\n",
            " [ 3.333577   -4.456604  ]\n",
            " [-2.5064538   2.7887588 ]\n",
            " [ 3.2011762  -4.273635  ]\n",
            " [-1.0317473   0.37960187]\n",
            " [-3.936554    4.196932  ]\n",
            " [ 3.3120868  -4.5273957 ]\n",
            " [-3.5216253   3.8486724 ]\n",
            " [ 2.957875   -3.9472964 ]\n",
            " [ 2.412898   -3.339176  ]\n",
            " [ 3.2869895  -4.3858323 ]\n",
            " [-3.9179082   4.2141414 ]\n",
            " [-3.8514671   4.1503263 ]\n",
            " [ 3.2634926  -4.397043  ]\n",
            " [ 3.1696246  -4.2901416 ]\n",
            " [-3.559469    3.819459  ]\n",
            " [ 3.0751233  -4.1397176 ]\n",
            " [-0.85216343  0.7287586 ]\n",
            " [-3.82908     4.150226  ]\n",
            " [-3.996052    4.3068757 ]\n",
            " [-3.9330618   4.262322  ]\n",
            " [ 3.2313004  -4.3650975 ]\n",
            " [-3.9380724   4.2458053 ]\n",
            " [-3.953133    4.303113  ]\n",
            " [-3.7598486   4.028785  ]\n",
            " [ 3.133637   -4.2541413 ]\n",
            " [-3.9598048   4.292856  ]\n",
            " [-3.5480785   3.9136672 ]\n",
            " [-3.8771951   4.2107472 ]\n",
            " [ 3.2325854  -4.3851256 ]\n",
            " [-3.9792485   4.2943473 ]\n",
            " [-3.752323    4.013026  ]\n",
            " [ 3.1644144  -4.2493896 ]\n",
            " [ 3.3345346  -4.406199  ]\n",
            " [ 3.301663   -4.4274406 ]\n",
            " [-3.1511683   3.425402  ]\n",
            " [ 1.8459754  -2.4528725 ]\n",
            " [-3.8123667   4.1381707 ]\n",
            " [-3.9340334   4.2429643 ]\n",
            " [-4.0104136   4.3439827 ]\n",
            " [-3.870573    4.2079387 ]\n",
            " [-3.943026    4.2454166 ]\n",
            " [ 3.2208688  -4.35384   ]\n",
            " [ 3.253963   -4.3912115 ]\n",
            " [-0.77806437  0.52304137]\n",
            " [ 2.8748667  -3.7414505 ]]\n",
            "[[ 3.0641265  -4.1330132 ]\n",
            " [ 3.1545222  -4.273335  ]\n",
            " [-0.6176568   0.24344495]\n",
            " [-3.9755175   4.285858  ]\n",
            " [ 3.1773908  -4.3674703 ]\n",
            " [ 3.1747782  -4.1934247 ]\n",
            " [ 2.6459103  -3.4351377 ]\n",
            " [ 3.000129   -4.0017166 ]\n",
            " [-3.22192     3.3745418 ]\n",
            " [ 3.156257   -4.1913414 ]\n",
            " [ 3.2557921  -4.3718357 ]\n",
            " [-4.0190954   4.3359804 ]\n",
            " [ 1.1981213  -1.4573878 ]\n",
            " [ 3.1501613  -4.287515  ]\n",
            " [ 3.3092122  -4.5089912 ]\n",
            " [ 3.1946688  -4.3455453 ]\n",
            " [-3.9382796   4.248697  ]\n",
            " [-3.92136     4.209133  ]\n",
            " [ 3.16333    -4.217657  ]\n",
            " [-2.1818936   2.2625728 ]\n",
            " [ 3.2354171  -4.2850094 ]\n",
            " [-3.9545588   4.2481213 ]\n",
            " [ 3.027882   -4.13898   ]\n",
            " [-3.9585497   4.2823114 ]\n",
            " [-3.1771555   3.3668501 ]\n",
            " [-4.0223083   4.355719  ]\n",
            " [-3.9038541   4.1913824 ]\n",
            " [ 2.9803627  -3.989661  ]\n",
            " [-3.1685758   3.4705505 ]\n",
            " [-3.9938822   4.3220935 ]\n",
            " [ 2.3759518  -3.200094  ]\n",
            " [ 3.208937   -4.3858757 ]\n",
            " [ 3.3121524  -4.413067  ]\n",
            " [ 3.106493   -4.205642  ]\n",
            " [-3.882584    4.15586   ]\n",
            " [ 3.362343   -4.4891996 ]\n",
            " [-3.909626    4.219047  ]\n",
            " [-3.764077    4.122736  ]\n",
            " [-3.5525017   3.9101322 ]\n",
            " [ 3.1355171  -4.333203  ]\n",
            " [ 3.3224502  -4.523181  ]\n",
            " [ 3.2377105  -4.416925  ]\n",
            " [-0.9776058   0.9957621 ]\n",
            " [ 3.3092062  -4.4660974 ]\n",
            " [-3.9824557   4.2995205 ]\n",
            " [ 3.3188965  -4.5215344 ]\n",
            " [-4.043927    4.3301826 ]\n",
            " [-2.7596924   2.9340239 ]\n",
            " [-3.9737074   4.291134  ]\n",
            " [ 3.1436572  -4.253357  ]\n",
            " [ 3.2366145  -4.3624105 ]\n",
            " [-3.959556    4.2655115 ]\n",
            " [-3.3762312   3.6663594 ]\n",
            " [ 2.8921618  -3.868919  ]\n",
            " [-0.7826446   0.7024759 ]\n",
            " [-4.0071044   4.3238826 ]\n",
            " [-3.8765504   4.2194877 ]\n",
            " [-3.910644    4.2424583 ]\n",
            " [-3.8604784   4.143701  ]\n",
            " [ 3.272362   -4.4163656 ]\n",
            " [-3.8825953   4.199965  ]\n",
            " [ 3.2032094  -4.341313  ]\n",
            " [ 3.328896   -4.405827  ]\n",
            " [-3.9083223   4.236636  ]]\n",
            "[[-3.885064   4.2007923]\n",
            " [ 3.3729427 -4.573844 ]\n",
            " [ 3.216267  -4.3647995]\n",
            " [ 2.3785272 -3.1380355]\n",
            " [ 3.3402576 -4.515959 ]\n",
            " [ 3.0231318 -4.040225 ]\n",
            " [-3.1284719  3.525107 ]\n",
            " [ 3.3441606 -4.4538736]\n",
            " [-3.6465847  3.9337373]\n",
            " [-3.6558962  4.0199857]\n",
            " [ 3.2456129 -4.3082876]\n",
            " [ 3.2676072 -4.366828 ]\n",
            " [-3.920767   4.2163663]\n",
            " [ 3.2674444 -4.36038  ]\n",
            " [ 3.2943373 -4.4847083]\n",
            " [-3.625675   3.825749 ]\n",
            " [-3.6779344  4.011138 ]\n",
            " [ 3.098696  -4.151828 ]\n",
            " [-4.0173507  4.304086 ]\n",
            " [-2.060111   2.2671545]\n",
            " [ 3.3734038 -4.543732 ]\n",
            " [-2.3309078  2.5325818]\n",
            " [ 3.2603903 -4.4070697]\n",
            " [-3.9991906  4.324026 ]\n",
            " [ 3.3029284 -4.484822 ]\n",
            " [-3.7514446  4.073179 ]\n",
            " [-3.9662642  4.2797   ]\n",
            " [-3.1604877  3.4920466]\n",
            " [ 3.1832685 -4.2703543]\n",
            " [ 2.5408578 -3.3158088]\n",
            " [ 3.3066826 -4.4644494]\n",
            " [-3.8607683  4.1523957]\n",
            " [ 3.273173  -4.4328475]\n",
            " [-3.7623463  4.123235 ]\n",
            " [-3.9394336  4.240497 ]\n",
            " [-3.8051229  4.070725 ]\n",
            " [ 3.277639  -4.3790193]\n",
            " [-4.0082684  4.365859 ]\n",
            " [ 3.2630718 -4.2810163]\n",
            " [ 3.3441038 -4.4243193]\n",
            " [ 3.3146248 -4.427478 ]\n",
            " [ 3.2160034 -4.393946 ]\n",
            " [-3.9766922  4.3104796]\n",
            " [ 3.3120868 -4.5273957]\n",
            " [-3.875079   4.1690893]\n",
            " [ 3.236642  -4.4130826]\n",
            " [ 3.1181207 -4.249171 ]\n",
            " [ 3.070864  -4.198036 ]\n",
            " [ 3.2446737 -4.366047 ]\n",
            " [-3.9658992  4.2615137]\n",
            " [-3.9852684  4.320248 ]\n",
            " [-3.9116757  4.236891 ]\n",
            " [-3.9387224  4.2735577]\n",
            " [-3.9974189  4.338674 ]\n",
            " [ 3.1609344 -4.218008 ]\n",
            " [-4.0277996  4.349251 ]\n",
            " [-3.42652    3.7033997]\n",
            " [-3.871648   4.0755544]\n",
            " [-3.9523456  4.2481856]\n",
            " [-3.8744383  4.202356 ]\n",
            " [-3.209985   3.544754 ]\n",
            " [-3.9602017  4.248942 ]\n",
            " [ 3.148684  -4.181712 ]\n",
            " [ 2.1188328 -2.8567696]]\n",
            "[[-3.8815134  4.175629 ]\n",
            " [-2.852618   3.1232972]\n",
            " [ 3.3290129 -4.385197 ]\n",
            " [ 3.31957   -4.4394875]\n",
            " [ 2.856123  -3.738668 ]\n",
            " [-4.036712   4.354675 ]\n",
            " [-3.9423378  4.22472  ]\n",
            " [-4.0033126  4.332164 ]\n",
            " [ 3.1990612 -4.3146663]\n",
            " [-3.9849544  4.3313704]\n",
            " [-3.0226629  3.3434772]\n",
            " [ 3.2001634 -4.292225 ]\n",
            " [-3.7052405  4.0171413]\n",
            " [-1.8500246  1.765271 ]\n",
            " [-3.902455   4.262188 ]\n",
            " [ 3.2861812 -4.3834085]\n",
            " [ 3.2683175 -4.4036007]\n",
            " [ 3.295592  -4.4027233]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4j6hRk0dV-x"
      },
      "source": [
        "predicted_class = []\n",
        "for x in range(len(logits)):\n",
        "  if(logits[x][0]>0):\n",
        "    abc=0\n",
        "  else:\n",
        "    abc=1\n",
        "  predicted_class.append(abc)\n",
        "predicted_class = np.array(predicted_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KewsAYEd70s"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os9KFzxigQPi",
        "outputId": "75b5bf06-77b7-4edc-e401-d62237b0d319"
      },
      "source": [
        "print(classification_report(label_ids, predicted_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.89      0.94         9\n",
            "           1       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.94        18\n",
            "   macro avg       0.95      0.94      0.94        18\n",
            "weighted avg       0.95      0.94      0.94        18\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLEFpQzAhFoT"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJC60wbdhl4e",
        "outputId": "7e743805-e3e8-48e5-fcc0-23bc5d420298"
      },
      "source": [
        "print(confusion_matrix(label_ids, predicted_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8 1]\n",
            " [0 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktlgte_lhp_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5b57db-41bd-4658-c309-a098e27e24f8"
      },
      "source": [
        "validation_df_1 = pd.read_excel(\"5000_new_Articles.xlsx\")\n",
        "validation_df_2 = pd.read_excel(\"other_5000_new_Articles.xlsx\")\n",
        "validation_df = pd.concat([validation_df_1, validation_df_2])\n",
        "validation_df = validation_df[['Title','abstract']]\n",
        "\n",
        "Abstracts = validation_df['Title']\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(cleanText(t))+['[SEP]'] , Abstracts))\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'es', '##cher', '##ichi', '##a', 'co', '##li', 'segments', 'its', 'controls', 'on', 'carbon', '-', 'dependent', 'gene', 'expression', 'into', 'global', 'and', 'specific', 'regulations', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prhH_0efXJzg"
      },
      "source": [
        "max_len = 100\n",
        "input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgSR5Xgeb2qA"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxzf6X2lcc6M",
        "outputId": "b7073759-9490-4a8c-f993-bb103df40138"
      },
      "source": [
        "len(validation_masks)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "466"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh9UDOGzcohN"
      },
      "source": [
        "batch_size = 64\n",
        "validation_inputs = torch.tensor(input_ids)\n",
        "validation_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RflY3TuDdkuE"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z8_WiImcyPF"
      },
      "source": [
        "all_predictions=[]\n",
        "for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
        "      logits = logits.detach().cpu().numpy()  \n",
        "      predicted_class = []\n",
        "      for x in range(len(logits)):\n",
        "        if(logits[x][0]>0):\n",
        "          abc=0\n",
        "        else:\n",
        "          abc=1\n",
        "        all_predictions.append(abc)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdHzPka4dz8-",
        "outputId": "6e8c307b-1a9b-47c3-f2b2-ac7718d17435"
      },
      "source": [
        "len(all_predictions)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jil7NEQeUku"
      },
      "source": [
        "validation_df[\"target\"]=all_predictions"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "fZWqnIJoeX9E",
        "outputId": "0ede31fa-7553-4195-bf04-6b3665d0a840"
      },
      "source": [
        "validation_df"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Escherichia coli segments its controls on carb...</td>\n",
              "      <td>How bacteria adjust gene expression to cope wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LC-MS/MS-based metabolic profiling of Escheric...</td>\n",
              "      <td>Escherichia coli is frequently exploited for g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reprogramming of gene expression in Escherichi...</td>\n",
              "      <td>Previous studies revealed important roles of s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Development of Escherichia coli-based gene exp...</td>\n",
              "      <td>Aims:                    The impact of municip...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Enhanced Phenyllactic Acid Production in Esche...</td>\n",
              "      <td>3-Phenyllactic acid (PhLA) is useful as a star...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>The promiscuous phosphomonoestearase activity ...</td>\n",
              "      <td>Membrane transport P-type ATPases display two ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>The CpxR/CpxA two-component system up-regulate...</td>\n",
              "      <td>We demonstrate that the twin arginine transloc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>Helicobacter pylori single-stranded DNA bindin...</td>\n",
              "      <td>Helicobacter pylori, an important bacterial pa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>Global gene expression during stringent respon...</td>\n",
              "      <td>Background:                    The stringent r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>Cloning, sequencing and expression of the gene...</td>\n",
              "      <td>The coenzyme B12 (adenosylcobalamin)-dependent...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  ... target\n",
              "0     Escherichia coli segments its controls on carb...  ...      1\n",
              "1     LC-MS/MS-based metabolic profiling of Escheric...  ...      0\n",
              "2     Reprogramming of gene expression in Escherichi...  ...      0\n",
              "3     Development of Escherichia coli-based gene exp...  ...      0\n",
              "4     Enhanced Phenyllactic Acid Production in Esche...  ...      0\n",
              "...                                                 ...  ...    ...\n",
              "4995  The promiscuous phosphomonoestearase activity ...  ...      0\n",
              "4996  The CpxR/CpxA two-component system up-regulate...  ...      1\n",
              "4997  Helicobacter pylori single-stranded DNA bindin...  ...      0\n",
              "4998  Global gene expression during stringent respon...  ...      1\n",
              "4999  Cloning, sequencing and expression of the gene...  ...      0\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "BiIFJua3eemm",
        "outputId": "63094b19-ad4f-40a9-bb1a-f9dda05731c1"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(all_predictions)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cce5e9610>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfDqgcZOerkf"
      },
      "source": [
        "validation_df.to_excel(\"Predicted_classification_10000.xlsx\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfcokKRdg5Lf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}